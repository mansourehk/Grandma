{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import transformers\r\n",
    "print(transformers.__version__)"
   ],
   "outputs": [],
   "metadata": {
    "id": "znvpwtsIMa1D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "base_dir = './'\r\n",
    "base_dir"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08KV7H3dN9Vu",
    "outputId": "4cb55174-2a68-4d67-a8e0-43397b32bbaa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import collections\r\n",
    "import json\r\n",
    "from transformers import BertTokenizer, BertForMaskedLM, DistilBertForSequenceClassification\r\n",
    "import re\r\n",
    "import string\r\n",
    "import logging\r\n",
    "import pickle\r\n",
    "from torch import nn\r\n",
    "import os\r\n",
    "from tqdm import tqdm\r\n",
    "import torch.nn.functional as F\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import spacy\r\n",
    "from spacy import displacy\r\n",
    "from spacy.lang.en import English\r\n",
    "import networkx as nx\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "\r\n",
    "\r\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "\r\n",
    "device = 'cuda:0'\r\n",
    "print(torch.cuda.get_device_name(device))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQE4y08YMGWi",
    "outputId": "306556e9-8646-4d81-b67e-119c0d94e030"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Root2Leaf Here ðŸ“„\n",
    "[Paper Link](https://arxiv.org/pdf/1703.00572.pdf)\n",
    "\n",
    "Basically the paper considers two different ways for integrating structure embeddings. For each word:\n",
    "1.   concat the word embedding (from GloVe) + passing the structure (from root to leaf) through the BiLSTM -> ```[Emb_word, h_fwd, h_bwd]```\n",
    "2.   concat the last hidden of dependences + structure -> ```[Emb_deps, h_fwd, h_bwd]```\n",
    "\n",
    "The second method achieves better results.\n",
    "\n",
    "the output of the following function:\n",
    "\n",
    "```{0: [[this, [('this', 'DT'), ('is', 'VBZ')]], [is, [('is', 'VBZ')]], [a, [('a', 'DT'), ('test', 'NN'), ('is', 'VBZ')]], [test, [('test', 'NN'), ('is', 'VBZ')]]]}```\n",
    "\n",
    "'0' is the sentence ID. For each word in the sentence we have a 2D list containing the actual word + dependencies and POS tags (the first dependancy is the actual word!).\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "EcfbIxp7R38-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def shortest_dependency_path(doc, e1=None, e2=None):\r\n",
    "    edges = []\r\n",
    "    for token in doc:\r\n",
    "        for child in token.children:\r\n",
    "            edges.append(('{0}'.format(token),\r\n",
    "                          '{0}'.format(child)))\r\n",
    "    graph = nx.Graph(edges)\r\n",
    "\r\n",
    "    try:\r\n",
    "        shortest_path = nx.shortest_path(graph, source=e1, target=e2)\r\n",
    "    except:\r\n",
    "        shortest_path = []\r\n",
    "\r\n",
    "    return shortest_path\r\n",
    "\r\n",
    "def dependency_tree(sents):\r\n",
    "    sent_deps = {}\r\n",
    "    sents = nlp(sents)\r\n",
    "\r\n",
    "    for s_ind, sentence in enumerate(sents.sents):\r\n",
    "        root = sentence.root\r\n",
    "        # print(root)\r\n",
    "        dictionary = dict()\r\n",
    "        all_dependency = {}\r\n",
    "        \r\n",
    "        for token in sentence:\r\n",
    "            if token.is_alpha:\r\n",
    "                dictionary[token.orth_] = token.tag_\r\n",
    "                # print(token.orth_, token.tag_, token.head.lemma_)\r\n",
    "                alist = shortest_dependency_path(sentence, token.orth_, str(root))\r\n",
    "                all_dependency[token] = alist\r\n",
    "        all_dependency_tag = []\r\n",
    "        for token, lists in all_dependency.items():\r\n",
    "            temp = collections.OrderedDict()\r\n",
    "            for item in lists:\r\n",
    "                if item in dictionary:\r\n",
    "                    temp[item] = dictionary[item]\r\n",
    "            all_dependency_tag.append([token, temp])\r\n",
    "        \r\n",
    "        # add to dict\r\n",
    "        sent_deps[s_ind] = all_dependency_tag\r\n",
    "\r\n",
    "    return sent_deps"
   ],
   "outputs": [],
   "metadata": {
    "id": "avMwlmMp9eY7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test the functions here\r\n",
    "print(dependency_tree('this is a test.'))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFalSunDSK3l",
    "outputId": "17d9e7c5-a6e3-4666-a1e6-1631996c6723"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create dictionary for POS tags! -> 1 time code\r\n",
    "if not os.path.exists(base_dir+'cache/tag_names.pkl'):\r\n",
    "    pos_tags = {}\r\n",
    "    tag_id = 0\r\n",
    "\r\n",
    "    for data in tqdm(dataset):\r\n",
    "        data = dataset.tokenizer.decode(data[0])\r\n",
    "        deps_dict = dependency_tree(data)\r\n",
    "        \r\n",
    "        for _, val in deps_dict.items():\r\n",
    "            # val is a 2D list\r\n",
    "            for sent_tags in val:\r\n",
    "                for tag_name in sent_tags:\r\n",
    "                    # add to dict\r\n",
    "                    pos_tags[tag_name] = pos_tags.setdefault(tag_name, tag_id)\r\n",
    "                    tag_id = max(pos_tags.values()) + 1\r\n",
    "    print('Num Tags = %d' %(len(pos_tags)))\r\n",
    "    pickle.dump(pos_tags, open(base_dir+'cache/tag_names.pkl', 'wb'))\r\n",
    "else:\r\n",
    "    pos_tags = pickle.load(open(base_dir+'cache/tag_names.pkl', 'rb'))\r\n",
    "    print('TagIDs loaded')"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pEBlh3XbVMW5",
    "outputId": "3ebfa2c2-3936-42d9-846d-64a3aabf035e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "emoji_pattern = re.compile(\"[\"\r\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
    "         u\"\\U00002702-\\U000027B0\"\r\n",
    "         u\"\\U000024C2-\\U0001F251\"\r\n",
    "         \"]+\", flags=re.UNICODE)\r\n",
    "\r\n",
    "emoticons = set([\r\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\r\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\r\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\r\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\r\n",
    "    '<3', ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\r\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\r\n",
    "    ':c', ':{', '>:\\\\', ';('\r\n",
    "    ])\r\n",
    "\r\n",
    "def clean_tweets(tweet, rm_puncs=True):\r\n",
    "    word_tokens = tweet.replace('\"', '').replace('<br />', '').replace(')', '').replace('(', '').lower().split(' ')\r\n",
    "    \r\n",
    "    #after tweepy preprocessing the colon symbol left remain after      #removing mentions\r\n",
    "    tweet = re.sub(r':', '', tweet)\r\n",
    "    tweet = re.sub(r'â€šÃ„Â¶', '', tweet)\r\n",
    "    \r\n",
    "    #replace consecutive non-ASCII characters with a space\r\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\r\n",
    "    \r\n",
    "    #remove emojis from tweet\r\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\r\n",
    "    \r\n",
    "    #looping through conditions\r\n",
    "    filtered_tweet = []\r\n",
    "    \r\n",
    "    for w in word_tokens:\r\n",
    "        #check tokens against stop words , emoticons and punctuations\r\n",
    "        if (w not in emoticons and not rm_puncs) or (rm_puncs and w not in string.punctuation and w not in emoticons):\r\n",
    "            filtered_tweet.append(w)\r\n",
    "            \r\n",
    "    return ' '.join(filtered_tweet)\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "CLGFDLp1MUDD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class RTDataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, filename='datasets/rotten_tomatoes_reviews.csv', rm_puncs=True, tokenizer=None, use_deps=False):\r\n",
    "        # load data from file\r\n",
    "        dataset_raw = pd.read_csv(filename)\r\n",
    "        col = ['Review', 'Freshness']\r\n",
    "        dataset_raw = dataset_raw[col]\r\n",
    "        # print(dataset_raw.head())\r\n",
    "        self.dataset = []\r\n",
    "        self.use_deps = use_deps\r\n",
    "        \r\n",
    "        # define tokenizer\r\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') if tokenizer == None else tokenizer\r\n",
    "        num_added_toks = self.tokenizer.add_tokens(['<pad>'])\r\n",
    "\r\n",
    "        if not os.path.exists(base_dir+'cache/rt_data_' + str(rm_puncs) + '.pkl'):\r\n",
    "            # convert and save + save the sentence structures as well\r\n",
    "            for data in dataset_raw.itertuples():\r\n",
    "                text, label = self.tokenizer.encode(clean_tweets(data[1], rm_puncs)), data[2]\r\n",
    "\r\n",
    "                if len(text) > 256:\r\n",
    "                    text = text[:256]\r\n",
    "                else:\r\n",
    "                    while len(text) < 256:\r\n",
    "                        text.append(self.tokenizer.encode('<pad>')[1])\r\n",
    "\r\n",
    "                d_text = self.tokenizer.decode(text)\r\n",
    "                self.dataset.append([\r\n",
    "                    text,\r\n",
    "                    dependency_tree(d_text[1 : len(d_text) - 1]),\r\n",
    "                    label\r\n",
    "                ])\r\n",
    "\r\n",
    "                # assert len(self.dataset[-1][0]) == 256\r\n",
    "                print('[%d/%d]                ' %(data[0] + 1, len(dataset_raw)), end='\\r', flush=True)  \r\n",
    "            pickle.dump(self.dataset, open(base_dir+'cache/rt_data_' + str(rm_puncs) + '.pkl', 'wb'))\r\n",
    "        else:\r\n",
    "            self.dataset = pickle.load(open(base_dir+'cache/rt_data_' + str(rm_puncs) + '.pkl', 'rb'))\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.dataset)\r\n",
    "        \r\n",
    "    def __getitem__(self, ind):\r\n",
    "        if not self.use_deps:\r\n",
    "            return torch.tensor(self.dataset[ind][0], dtype=torch.float32), self.dataset[ind][2], self.dataset[ind][1]\r\n",
    "        \r\n",
    "        # else, get everything from the tree\r\n",
    "        sent = []\r\n",
    "        dependencies = []\r\n",
    "\r\n",
    "        for sentence in self.dataset[ind][1]:\r\n",
    "            # loop on words\r\n",
    "            for word, deps in sentence:\r\n",
    "                sent.append(word)\r\n",
    "                dependencies.append( self.tokenizer.encode(list(deps.keys())) )\r\n",
    "\r\n",
    "        # convert to vector\r\n",
    "        sent = torch.tensor( self.tokenizer.encode(sent) )\r\n",
    "\r\n",
    "        return sent, self.dataset[ind][2], dependencies"
   ],
   "outputs": [],
   "metadata": {
    "id": "mksBJ4C91DOB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class IMDBDataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, filename='datasets/IMDB Dataset.csv', rm_puncs=True, tokenizer=None, use_deps=False, max_K=None):\r\n",
    "        # load data from file\r\n",
    "        dataset_raw = pd.read_csv(filename)\r\n",
    "        self.dataset = []\r\n",
    "        \r\n",
    "        # define tokenizer\r\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') if tokenizer == None else tokenizer\r\n",
    "        num_added_toks = self.tokenizer.add_tokens(['<pad>'])\r\n",
    "        self.use_deps = use_deps\r\n",
    "        self.K = max_K\r\n",
    "\r\n",
    "        if use_deps:\r\n",
    "            assert max_K != None, 'Please enter a valid K!'\r\n",
    "\r\n",
    "        if not os.path.exists(base_dir+'cache/imdb_data_' + str(rm_puncs) + '.pkl'):\r\n",
    "            # convert and save + save the sentence structures as well\r\n",
    "            for data in dataset_raw.itertuples():\r\n",
    "                text, label = self.tokenizer.encode(clean_tweets(data[1], rm_puncs)), data[2]\r\n",
    "\r\n",
    "                if len(text) > 256:\r\n",
    "                    text = text[:256]\r\n",
    "                else:\r\n",
    "                    while len(text) < 256:\r\n",
    "                        text.append(self.tokenizer.encode('<pad>')[1])\r\n",
    "\r\n",
    "                d_text = self.tokenizer.decode(text)\r\n",
    "                self.dataset.append([\r\n",
    "                    text,\r\n",
    "                    dependency_tree(d_text[1 : len(d_text) - 1]),\r\n",
    "                    1 if label == 'positive' else 0\r\n",
    "                ])\r\n",
    "\r\n",
    "                # assert len(self.dataset[-1][0]) == 256\r\n",
    "                print('[%d/%d]                ' %(data[0] + 1, len(dataset_raw)), end='\\r', flush=True)  \r\n",
    "            pickle.dump(self.dataset, open(base_dir+'cache/imdb_data_' + str(rm_puncs) + '.pkl', 'wb'))\r\n",
    "        else:\r\n",
    "            self.dataset = pickle.load(open(base_dir+'cache/imdb_data_' + str(rm_puncs) + '.pkl', 'rb'))\r\n",
    "            # self.create_cache(rm_puncs)\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.dataset)\r\n",
    "\r\n",
    "    def create_cache(self, rm_puncs):\r\n",
    "        # lol this function creates a cache from another cache :| \r\n",
    "        self.new_dataset = []\r\n",
    "        if not os.path.exists(base_dir+'cache/imdb_data_' + str(rm_puncs) + '_cache' + '.pkl'):\r\n",
    "            for ind in range(len(self.dataset)):\r\n",
    "                sent = []\r\n",
    "                dependencies = [ [self.tokenizer.encode('<sos>')[1] for i in range(self.K)] ]\r\n",
    "\r\n",
    "                for sentence in self.dataset[ind][1]:\r\n",
    "                    # loop on words\r\n",
    "                    for word, deps in sentence:\r\n",
    "                        pattern = r\"\\(\\'\\S+\\'\" # \\'\\S+\\'\\))\"\r\n",
    "                        words = re.findall(pattern, deps)\r\n",
    "\r\n",
    "                        sent.append(word)\r\n",
    "                        a = [w[2:-1] for w in words]\r\n",
    "                        if a != []:\r\n",
    "                            deps = self.tokenizer.encode(a)[1:]\r\n",
    "                        else:\r\n",
    "                            deps = [ self.tokenizer.encode('<pad>')[1] for i in range(self.K) ]    \r\n",
    "                        \r\n",
    "                        if len(deps) > self.K:\r\n",
    "                            deps = deps[:self.K]\r\n",
    "                        else:\r\n",
    "                            while len(deps) < self.K:\r\n",
    "                                deps.append(self.tokenizer.encode('<pad>')[1])\r\n",
    "                        \r\n",
    "                        dependencies.append(deps)\r\n",
    "\r\n",
    "                dependencies.append([self.tokenizer.encode('<pad>')[1] for i in range(self.K)])\r\n",
    "                # convert to vector\r\n",
    "                sent = self.tokenizer.encode(sent)\r\n",
    "                if len(sent) > 256:\r\n",
    "                    sent = sent[:256]\r\n",
    "                    dependencies = dependencies[:256]\r\n",
    "                else:\r\n",
    "                    while len(sent) < 256:\r\n",
    "                        sent.append(self.tokenizer.encode('<pad>')[1])\r\n",
    "                        dependencies.append( [ self.tokenizer.encode('<pad>')[1] for i in range(self.K) ] )\r\n",
    "\r\n",
    "                self.new_dataset.append( (sent, self.dataset[ind][2], dependencies) )\r\n",
    "                if ind % 1000 == 0:\r\n",
    "                    print('[%d/%d]        ' %(ind + 1, len(self.dataset)))  \r\n",
    "            self.dataset = self.new_dataset\r\n",
    "            pickle.dump(self.dataset, open(base_dir+'cache/imdb_data_' + str(rm_puncs) + '_cache' + '.pkl', 'wb'))\r\n",
    "        else:\r\n",
    "            self.dataset = pickle.load(open(base_dir+'cache/imdb_data_' + str(rm_puncs) + '_cache' + '.pkl', 'rb'))\r\n",
    "\r\n",
    "    def __getitem__(self, ind):\r\n",
    "        return torch.tensor(self.dataset[ind][0], dtype=torch.float32), self.dataset[ind][2], torch.tensor(self.dataset[ind][1])"
   ],
   "outputs": [],
   "metadata": {
    "id": "zL7TnWJYMVyw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class IMDBDataset_NEW(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, filename='datasets/aclimdb_train.csv', rm_puncs=True, tokenizer=None, use_deps=False, max_K=None, bert_mode=False):\r\n",
    "        # load data from file\r\n",
    "        dataset_raw = pd.read_csv(filename)\r\n",
    "        self.dataset = []\r\n",
    "        self.bert_mode = bert_mode\r\n",
    "        \r\n",
    "        # define tokenizer\r\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') if tokenizer == None else tokenizer\r\n",
    "        num_added_toks = self.tokenizer.add_tokens(['<pad>'])\r\n",
    "        self.use_deps = use_deps\r\n",
    "        self.K = max_K\r\n",
    "\r\n",
    "        if use_deps:\r\n",
    "            assert max_K != None, 'Please enter a valid K!'\r\n",
    "\r\n",
    "        if not os.path.exists(base_dir+'cache/imdb_data_' + str(rm_puncs) + '.pkl'):\r\n",
    "            # convert and save + save the sentence structures as well\r\n",
    "            for data in dataset_raw.itertuples():\r\n",
    "                text, label = self.tokenizer.encode(clean_tweets(data[1], rm_puncs)), data[2]\r\n",
    "\r\n",
    "                if len(text) > 256:\r\n",
    "                    text = text[:256]\r\n",
    "                else:\r\n",
    "                    while len(text) < 256:\r\n",
    "                        text.append(self.tokenizer.encode('<pad>')[1])\r\n",
    "\r\n",
    "                d_text = self.tokenizer.decode(text)\r\n",
    "                self.dataset.append([\r\n",
    "                    text,\r\n",
    "                    dependency_tree(d_text[1 : len(d_text) - 1]),\r\n",
    "                    1 if label == 'positive' else 0\r\n",
    "                ])\r\n",
    "\r\n",
    "                # assert len(self.dataset[-1][0]) == 256\r\n",
    "                print('[%d/%d]                ' %(data[0] + 1, len(dataset_raw)), end='\\r', flush=True)  \r\n",
    "            pickle.dump(self.dataset, open(base_dir+'cache/imdb_data_' + str(rm_puncs) + '.pkl', 'wb'))\r\n",
    "        else:\r\n",
    "            self.dataset = pickle.load(open(base_dir+'cache/imdb_data_' + str(rm_puncs) + '.pkl', 'rb'))\r\n",
    "            # self.create_cache(rm_puncs)\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return len(self.dataset)\r\n",
    "\r\n",
    "    def create_cache(self, rm_puncs):\r\n",
    "        # lol this function creates a cache from another cache :| \r\n",
    "        self.new_dataset = []\r\n",
    "        if not os.path.exists(base_dir+'cache/imdb_data_' + str(rm_puncs) + '_cache' + '.pkl'):\r\n",
    "            for ind in range(len(self.dataset)):\r\n",
    "                sent = []\r\n",
    "                dependencies = [ [self.tokenizer.encode('<sos>')[1] for i in range(self.K)] ]\r\n",
    "\r\n",
    "                for sentence in self.dataset[ind][1]:\r\n",
    "                    # loop on words\r\n",
    "                    for word, deps in sentence:\r\n",
    "                        pattern = r\"\\(\\'\\S+\\'\" # \\'\\S+\\'\\))\"\r\n",
    "                        words = re.findall(pattern, deps)\r\n",
    "\r\n",
    "                        sent.append(word)\r\n",
    "                        a = [w[2:-1] for w in words]\r\n",
    "                        if a != []:\r\n",
    "                            deps = self.tokenizer.encode(a)[1:]\r\n",
    "                        else:\r\n",
    "                            deps = [ self.tokenizer.encode('<pad>')[1] for i in range(self.K) ]    \r\n",
    "                        \r\n",
    "                        if len(deps) > self.K:\r\n",
    "                            deps = deps[:self.K]\r\n",
    "                        else:\r\n",
    "                            while len(deps) < self.K:\r\n",
    "                                deps.append(self.tokenizer.encode('<pad>')[1])\r\n",
    "                        \r\n",
    "                        dependencies.append(deps)\r\n",
    "\r\n",
    "                dependencies.append([self.tokenizer.encode('<pad>')[1] for i in range(self.K)])\r\n",
    "                # convert to vector\r\n",
    "                sent = self.tokenizer.encode(sent)\r\n",
    "                if len(sent) > 256:\r\n",
    "                    sent = sent[:256]\r\n",
    "                    dependencies = dependencies[:256]\r\n",
    "                else:\r\n",
    "                    while len(sent) < 256:\r\n",
    "                        sent.append(self.tokenizer.encode('<pad>')[1])\r\n",
    "                        dependencies.append( [ self.tokenizer.encode('<pad>')[1] for i in range(self.K) ] )\r\n",
    "\r\n",
    "                self.new_dataset.append( (sent, self.dataset[ind][2], dependencies) )\r\n",
    "                if ind % 1000 == 0:\r\n",
    "                    print('[%d/%d]        ' %(ind + 1, len(self.dataset)))  \r\n",
    "            self.dataset = self.new_dataset\r\n",
    "            pickle.dump(self.dataset, open(base_dir+'cache/imdb_data_' + str(rm_puncs) + '_cache' + '.pkl', 'wb'))\r\n",
    "        else:\r\n",
    "            print('loading from cache')\r\n",
    "            self.dataset = pickle.load(open(base_dir+'cache/imdb_data_' + str(rm_puncs) + '_cache' + '.pkl', 'rb'))\r\n",
    "\r\n",
    "    def __getitem__(self, ind):\r\n",
    "        if not self.bert_mode:\r\n",
    "            return torch.tensor(self.dataset[ind][0], dtype=torch.float32), self.dataset[ind][2], torch.tensor(self.dataset[ind][1])\r\n",
    "        else:\r\n",
    "            encoded_sent = [self.dataset[ind][0]]\r\n",
    "            input_ids = pad_sequences(encoded_sent, maxlen=512, dtype=\"long\", truncating=\"post\", padding=\"post\")\r\n",
    "            a = self.tokenizer.encode('<pad>')[1]\r\n",
    "            seq_mask = [float(i!=a) for i in encoded_sent[0]]\r\n",
    "            # Convert to tensors.\r\n",
    "            while a in encoded_sent[0]:\r\n",
    "                encoded_sent[0][encoded_sent[0].index(a)] = 0\r\n",
    "\r\n",
    "            for ind, i in enumerate(encoded_sent[0]):   \r\n",
    "                s = ',.!?'             \r\n",
    "                if self.tokenizer.decode([i]) in s:\r\n",
    "                    encoded_sent[0][ind] = dataset.tokenizer.encode( s[random.randint(0, 3)] )[1]\r\n",
    "\r\n",
    "            prediction_inputs = torch.tensor(encoded_sent[0])\r\n",
    "            prediction_masks = torch.tensor(seq_mask)\r\n",
    "\r\n",
    "            return prediction_inputs, prediction_masks, torch.tensor(self.dataset[ind][2])"
   ],
   "outputs": [],
   "metadata": {
    "id": "RGXJW3m8UWKi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "punc = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # test classes\r\n",
    "\r\n",
    "# dataset = IMDBDataset(base_dir+'datasets/IMDB Dataset.csv', punc, use_deps=True, max_K=5)\r\n",
    "# train_size = int(0.5 * len(dataset))\r\n",
    "# test_size = len(dataset) - train_size\r\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\r\n",
    "\r\n",
    "# valid_size = int(0.2 * len(train_dataset))\r\n",
    "# valid_dataset, _ = torch.utils.data.random_split(train_dataset, [valid_size, train_size - valid_size])\r\n",
    "# del _\r\n",
    "\r\n",
    "# # convert data to batches\r\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128)\r\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128)\r\n",
    "# valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=128)\r\n",
    "\r\n",
    "# print('Train = %d (%d) | Test = %d (%d) | Validation = %d (%d)' %(len(train_dataset), len(train_loader), len(test_dataset), len(test_loader), len(valid_dataset), len(valid_loader)))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gFaeGpZQo0Mq",
    "outputId": "19eb18d1-5e3b-4e5a-e018-21a6506a019b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = IMDBDataset_NEW(base_dir+'datasets/aclimdb.csv', punc, use_deps=True, max_K=5, bert_mode=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = IMDBDataset_NEW(base_dir+'datasets/aclimdb_train.csv', punc, use_deps=True, max_K=5, bert_mode=True)"
   ],
   "outputs": [],
   "metadata": {
    "id": "jMkl_928Wft5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_dataset = IMDBDataset_NEW(base_dir+'datasets/aclimdb_test.csv', punc, use_deps=True, max_K=5, bert_mode=True)"
   ],
   "outputs": [],
   "metadata": {
    "id": "qVAAtYWqWhmJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test classes\r\n",
    "\r\n",
    "# train_size = int(0.5 * len(dataset))\r\n",
    "# test_size = len(dataset) - train_size\r\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\r\n",
    "train_size = len(train_dataset)\r\n",
    "valid_size = int(0.2 * len(train_dataset))\r\n",
    "valid_dataset, _ = torch.utils.data.random_split(train_dataset, [valid_size, train_size - valid_size])\r\n",
    "del _\r\n",
    "\r\n",
    "# convert data to batches\r\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\r\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\r\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32)\r\n",
    "\r\n",
    "print('Train = %d (%d) | Test = %d (%d) | Validation = %d (%d)' %(len(train_dataset), len(train_loader), len(test_dataset), len(test_loader), len(valid_dataset), len(valid_loader)))\r\n",
    "\r\n",
    "dataset[0]"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589,
     "referenced_widgets": [
      "bb67626da99843a7b8c14fd869e85080",
      "bda0de59590344708c2b60dca090c687",
      "3f17cf9c83674d1fb822329fa6d5e015",
      "ca68dd3284ac4965baa4a5496ffbc663",
      "c9240036b41442c082fec7162d249713",
      "cb48191c59714b58ba38e49e16ea77e3",
      "28195f49042d4151b8191ceee753749c",
      "40449512fcae427eabfb6af1f5e4908c",
      "c234c2824a204609be6af46e974881be",
      "681e0f4c216d4a69baa4694c985bf18a",
      "15fd4d631ae34c36be18fa08f9ba2457",
      "21740080bc6b4e8eba7b412261c37e6f",
      "4c085f8a4006497c95722f9225748282",
      "59ee1a52c1d7433189e015b8a21d7d1b",
      "5a8df54d5d9d4207acd7c9a1780a9d9c",
      "6368c08d69b44d75a9ab5eb96f43d86e",
      "96e04dfd07f2429b9b0fbb4cb133a574",
      "b6fa1e8c8243406cbabae8f205e717fb",
      "f7d250b016064514a0b45353ccdb34e9",
      "99cfbff75de34278a9201ad8e72b7505",
      "fe7c4a0c4694467fa7024d1b07459084",
      "4fecbe7bf0614d9094680af0c603d252",
      "bf51472b91964e83ae23735c2cc692df",
      "097695a91986466c848d6c31bf925898"
     ]
    },
    "id": "XA8tJHvnU7H5",
    "outputId": "924ad2c7-5471-4979-c387-5a534448c499"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BiGRU Classifier Here\n",
    "can be used with the same training code as the \"+Attention\" one"
   ],
   "metadata": {
    "id": "PxwRqqkuULed"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class biGRU_classifier(nn.Module):\r\n",
    "    def __init__(self, vocab_size, pad_idx, vocab):\r\n",
    "        super(biGRU_classifier, self).__init__()\r\n",
    "        \r\n",
    "        # load glove and initialize\r\n",
    "        if not os.path.exists(base_dir+'cache/biGRU_embeddings.pt'):\r\n",
    "            glove_dict = {}\r\n",
    "\r\n",
    "            with open(base_dir+'datasets/glove.6B.100d.txt', 'r', encoding='utf-8') as fin:\r\n",
    "                for line in fin.readlines():\r\n",
    "                    line = line.split(' ')\r\n",
    "                    glove_dict[line[0]] = torch.tensor(list(map(float, line[1:])))\r\n",
    "\r\n",
    "            # create embedding matrix\r\n",
    "            emb_mat = torch.randn((vocab_size, 100))\r\n",
    "            for word, w_emb in glove_dict.items():\r\n",
    "                emb_mat[vocab.encode(word)[1]] = w_emb\r\n",
    "            torch.save(emb_mat, base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        else:\r\n",
    "            emb_mat = torch.load(base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        \r\n",
    "        # define layers\r\n",
    "        self.emb = nn.Embedding(vocab_size, 100, padding_idx=pad_idx)\r\n",
    "        self.emb.weight.data.copy_(emb_mat)\r\n",
    "        self.bi_GRU = nn.GRU(100, 256, 1, batch_first=True, bidirectional=True)\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.Linear(512, 128),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(128, 2),\r\n",
    "            nn.Softmax(dim=-1)\r\n",
    "        )\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.emb(x.long())\r\n",
    "        _, x = self.bi_GRU(x)\r\n",
    "        x = torch.cat((x[0], x[1]), dim=-1)\r\n",
    "        \r\n",
    "        return self.classifier(x)\r\n",
    "    \r\n",
    "    def forward_grad(self, x):\r\n",
    "        x = self.emb(x.long())\r\n",
    "        \r\n",
    "        _, x = self.bi_GRU(x)\r\n",
    "        x = torch.cat((x[0], x[1]), dim=-1)\r\n",
    "        \r\n",
    "        return self.classifier(x)"
   ],
   "outputs": [],
   "metadata": {
    "id": "PWYskC68ULiL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the BiGRU+Attention Classifier "
   ],
   "metadata": {
    "id": "tz7qsC29RxjK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AttentionLayer(nn.Module):\r\n",
    "    def __init__(self, emb_dim, device):\r\n",
    "        super(AttentionLayer, self).__init__()\r\n",
    "    \r\n",
    "        # define attentiton layer here\r\n",
    "        self.att_layer = torch.randn((emb_dim, 1), requires_grad=True).to(device)\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        # x.shape = [batch, seq_len, emb_size]\r\n",
    "        x_p = torch.tanh(x)\r\n",
    "        x_p = torch.bmm(x_p, self.att_layer.repeat((x_p.shape[0], 1, 1)))\r\n",
    "        x_p = torch.functional.F.softmax(x_p, dim=-2)\r\n",
    "        return x_p, torch.tanh(torch.bmm(x.transpose(1, 2), x_p).squeeze(-1))\r\n",
    "    \r\n",
    "class biGRUAttn_classifier(nn.Module):\r\n",
    "    def __init__(self, vocab_size, pad_idx, vocab):\r\n",
    "        super(biGRUAttn_classifier, self).__init__()\r\n",
    "        \r\n",
    "        # load glove and initialize\r\n",
    "        if not os.path.exists(base_dir+'cache/biGRU_embeddings.pt'):\r\n",
    "            glove_dict = {}\r\n",
    "\r\n",
    "            with open(base_dir+'datasets/glove.6B.100d.txt', 'r', encoding='utf-8') as fin:\r\n",
    "                for line in fin.readlines():\r\n",
    "                    line = line.split(' ')\r\n",
    "                    glove_dict[line[0]] = torch.tensor(list(map(float, line[1:])))\r\n",
    "\r\n",
    "            # create embedding matrix\r\n",
    "            emb_mat = torch.randn((vocab_size, 100))\r\n",
    "            for word, w_emb in glove_dict.items():\r\n",
    "                emb_mat[vocab.encode(word)[1]] = w_emb\r\n",
    "            torch.save(emb_mat, base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        else:\r\n",
    "            emb_mat = torch.load(base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        \r\n",
    "        # define layers\r\n",
    "        self.emb = nn.Embedding(vocab_size, 100, padding_idx=pad_idx)\r\n",
    "        self.emb.weight.data.copy_(emb_mat)\r\n",
    "        self.bi_GRU = nn.GRU(100, 256, 1, batch_first=True, bidirectional=True)\r\n",
    "        self.attention = AttentionLayer(512, device)\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.Linear(512, 128),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(128, 2),\r\n",
    "            nn.Softmax(dim=-1)\r\n",
    "        )\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.emb(x.long())\r\n",
    "        outputs, hidden = self.bi_GRU(x)\r\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\r\n",
    "        \r\n",
    "        # pass to the attention module\r\n",
    "        scores, context = self.attention(outputs)\r\n",
    "        \r\n",
    "        return scores, self.classifier(context)"
   ],
   "outputs": [],
   "metadata": {
    "id": "-P0BSLFBo1vw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_on_data(data_loader, model):\r\n",
    "    model.eval()\r\n",
    "    true_labels = []\r\n",
    "    pred_labels = []\r\n",
    "    \r\n",
    "    for i, data in enumerate(data_loader):\r\n",
    "        # get the inputs; data is a list of [inputs, labels]\r\n",
    "        inputs, labels, _ = data\r\n",
    "\r\n",
    "        # forward + backward + optimize\r\n",
    "        _, outputs = model(inputs.to(device))\r\n",
    "        # outputs = model(inputs.to(device))\r\n",
    "        \r\n",
    "        true_labels.extend(labels.tolist())\r\n",
    "        pred_labels.extend(torch.argmax(outputs, dim=-1).tolist())\r\n",
    "        \r\n",
    "    # get confusion matrix\r\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()\r\n",
    "    return (tp + tn) / (tn + fp + fn + tp)"
   ],
   "outputs": [],
   "metadata": {
    "id": "bskHztkS9T8R"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name = 'biGRUAttn_model_newIMDB_False.pt'\r\n",
    "\r\n",
    "if not os.path.exists(base_dir+'cache/' + model_name):\r\n",
    "    vocab_size = len(dataset.tokenizer.get_vocab())\r\n",
    "    # net = biGRU_classifier(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).to(device)\r\n",
    "    net = biGRUAttn_classifier(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).to(device)\r\n",
    "    criterion = nn.CrossEntropyLoss()\r\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\r\n",
    "    print('Start training ...')\r\n",
    "    for epoch in range(20):\r\n",
    "        net.train()\r\n",
    "\r\n",
    "        for i, data in enumerate(train_loader):\r\n",
    "            # get the inputs; data is a list of [inputs, labels]\r\n",
    "            inputs, labels, _ = data\r\n",
    "\r\n",
    "            # zero the parameter gradients\r\n",
    "            optimizer.zero_grad()\r\n",
    "\r\n",
    "            # forward + backward + optimize\r\n",
    "            _, outputs = net(inputs.to(device))\r\n",
    "            # outputs = net(inputs.to(device))\r\n",
    "            loss = criterion(outputs, labels.to(device))\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "        # test on data\r\n",
    "        with torch.no_grad():\r\n",
    "            print('[%d/%d] Validation = %0.4f | Test = %0.4f' %(\r\n",
    "                epoch + 1,\r\n",
    "                30,\r\n",
    "                test_on_data(valid_loader, net),\r\n",
    "                test_on_data(test_loader, net)\r\n",
    "            ))\r\n",
    "\r\n",
    "    print('Finished Training')\r\n",
    "    torch.save(net.state_dict(), base_dir+'cache/' + model_name)\r\n",
    "else:\r\n",
    "    vocab_size = len(dataset.tokenizer.get_vocab())\r\n",
    "    # net = biGRU_classifier(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).float().to(device)\r\n",
    "    net = biGRUAttn_classifier(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).to(device)\r\n",
    "    net.load_state_dict(torch.load(base_dir+'cache/' + model_name))\r\n",
    "    print('Model Loaded')\r\n",
    "    print(test_on_data(test_loader, net))\r\n",
    "    print(test_on_data(train_loader, net))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "jq8VMbhn9WQg",
    "outputId": "167e8b7e-2f50-42e3-94d8-8ee505757239"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_on_data_gru(data_loader, model):\r\n",
    "    model.eval()\r\n",
    "    true_labels = []\r\n",
    "    pred_labels = []\r\n",
    "    \r\n",
    "    for i, data in enumerate(data_loader):\r\n",
    "        # get the inputs; data is a list of [inputs, labels]\r\n",
    "        inputs, labels, _ = data\r\n",
    "\r\n",
    "        # forward + backward + optimize\r\n",
    "#         _, outputs = model(inputs.to(device))\r\n",
    "        outputs = model(inputs.to(device))\r\n",
    "        \r\n",
    "        true_labels.extend(labels.tolist())\r\n",
    "        pred_labels.extend(torch.argmax(outputs, dim=-1).tolist())\r\n",
    "        \r\n",
    "    # get confusion matrix\r\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()\r\n",
    "    return (tp + tn) / (tn + fp + fn + tp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name = 'biGRU_model_newIMDB_False.pt'\r\n",
    "\r\n",
    "if not os.path.exists(base_dir+'cache/' + model_name):\r\n",
    "    vocab_size = len(dataset.tokenizer.get_vocab())\r\n",
    "    net = biGRU_classifier(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).to(device)\r\n",
    "#     net = biGRUAttn_classifier(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).to(device)\r\n",
    "    criterion = nn.CrossEntropyLoss()\r\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\r\n",
    "    print('Start training ...')\r\n",
    "    for epoch in range(30):\r\n",
    "        net.train()\r\n",
    "\r\n",
    "        for i, data in enumerate(train_loader):\r\n",
    "            # get the inputs; data is a list of [inputs, labels]\r\n",
    "            inputs, labels, _ = data\r\n",
    "\r\n",
    "            # zero the parameter gradients\r\n",
    "            optimizer.zero_grad()\r\n",
    "\r\n",
    "            # forward + backward + optimize\r\n",
    "#             _, outputs = net(inputs.to(device))\r\n",
    "            outputs = net(inputs.to(device))\r\n",
    "            loss = criterion(outputs, labels.to(device))\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "        # test on data\r\n",
    "        with torch.no_grad():\r\n",
    "            print('[%d/%d] Validation = %0.4f | Test = %0.4f' %(\r\n",
    "                epoch + 1,\r\n",
    "                30,\r\n",
    "                test_on_data_gru(valid_loader, net),\r\n",
    "                test_on_data_gru(test_loader, net)\r\n",
    "            ))\r\n",
    "\r\n",
    "    print('Finished Training')\r\n",
    "    torch.save(net.state_dict(), base_dir+'cache/' + model_name)\r\n",
    "else:\r\n",
    "    vocab_size = len(dataset.tokenizer.get_vocab())\r\n",
    "    net = biGRU_classifier(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).float().to(device)\r\n",
    "#     net = biGRUAttn_classifier(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).to(device)\r\n",
    "    net.load_state_dict(torch.load(base_dir+'cache/' + model_name))\r\n",
    "    print('Model Loaded')\r\n",
    "    print(test_on_data_gru(test_loader, net))\r\n",
    "    print(test_on_data_gru(train_loader, net))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Proposed Model\n",
    "\n",
    "PunClassifier_1: uses BiGRU + Attention for the sentence embedding and a simple BiGRU for the structure embedding + a neural network for combining the two embeddings\n",
    "\n"
   ],
   "metadata": {
    "id": "flnWmDBYS4_1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AttentionLayer(nn.Module):\r\n",
    "    def __init__(self, emb_dim, device):\r\n",
    "        super(AttentionLayer, self).__init__()\r\n",
    "    \r\n",
    "        # define attentiton layer here\r\n",
    "        self.att_layer = torch.randn((emb_dim, 1), requires_grad=True).to(device)\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        # x.shape = [batch, seq_len, emb_size]\r\n",
    "        x_p = torch.tanh(x)\r\n",
    "        x_p = torch.bmm(x_p, self.att_layer.repeat((x_p.shape[0], 1, 1)))\r\n",
    "        x_p = torch.functional.F.softmax(x_p, dim=-2)\r\n",
    "        return x_p, torch.tanh(torch.bmm(x.transpose(1, 2), x_p).squeeze(-1))\r\n",
    "\r\n",
    "class BaseEmbedding_1(nn.Module):\r\n",
    "    def __init__(self, vocab_size, pad_idx):\r\n",
    "        super(BaseEmbedding_1, self).__init__()\r\n",
    "\r\n",
    "        # load glove and initialize\r\n",
    "        if not os.path.exists(base_dir+'cache/biGRU_embeddings.pt'):\r\n",
    "            glove_dict = {}\r\n",
    "\r\n",
    "            with open(base_dir+'datasets/glove.6B.100d.txt', 'r', encoding='utf-8') as fin:\r\n",
    "                for line in fin.readlines():\r\n",
    "                    line = line.split(' ')\r\n",
    "                    glove_dict[line[0]] = torch.tensor(list(map(float, line[1:])))\r\n",
    "\r\n",
    "            # create embedding matrix\r\n",
    "            emb_mat = torch.randn((vocab_size, 100))\r\n",
    "            for word, w_emb in glove_dict.items():\r\n",
    "                emb_mat[vocab.encode(word)[1]] = w_emb\r\n",
    "            torch.save(emb_mat, base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        else:\r\n",
    "            emb_mat = torch.load(base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        \r\n",
    "        # define layers\r\n",
    "        self.emb = nn.Embedding(vocab_size, 100, padding_idx=pad_idx)\r\n",
    "        self.emb.weight.data.copy_(emb_mat)\r\n",
    "        self.bi_GRU = nn.GRU(100, 10, 1, batch_first=True, bidirectional=True)\r\n",
    "\r\n",
    "    def forward(self, deps):\r\n",
    "        # deps -> [batch_size, max_seq_len, K] -> the focus us on K\r\n",
    "        deps_emb = torch.empty((deps.shape[0], deps.shape[1], 20)).to(device)\r\n",
    "\r\n",
    "        for i in range(deps.size(1)):\r\n",
    "            x = self.emb(deps[:, i, :].long())\r\n",
    "            outputs, hidden = self.bi_GRU(x)\r\n",
    "            # hidden -> [batch_size, emb_size (20)]\r\n",
    "            hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\r\n",
    "            deps_emb[:, i, :] = hidden\r\n",
    "\r\n",
    "        return deps_emb\r\n",
    "\r\n",
    "\r\n",
    "class BaseClassifier_1(nn.Module):\r\n",
    "    def __init__(self, vocab_size, pad_idx, vocab):\r\n",
    "        super(BaseClassifier_1, self).__init__()\r\n",
    "        \r\n",
    "        # load glove and initialize\r\n",
    "        if not os.path.exists(base_dir+'cache/biGRU_embeddings.pt'):\r\n",
    "            glove_dict = {}\r\n",
    "\r\n",
    "            with open(base_dir+'datasets/glove.6B.100d.txt', 'r', encoding='utf-8') as fin:\r\n",
    "                for line in fin.readlines():\r\n",
    "                    line = line.split(' ')\r\n",
    "                    glove_dict[line[0]] = torch.tensor(list(map(float, line[1:])))\r\n",
    "\r\n",
    "            # create embedding matrix\r\n",
    "            emb_mat = torch.randn((vocab_size, 100))\r\n",
    "            for word, w_emb in glove_dict.items():\r\n",
    "                emb_mat[vocab.encode(word)[1]] = w_emb\r\n",
    "            torch.save(emb_mat, base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        else:\r\n",
    "            emb_mat = torch.load(base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        \r\n",
    "        # define layers\r\n",
    "        self.emb = nn.Embedding(vocab_size, 100, padding_idx=pad_idx)\r\n",
    "        self.emb.weight.data.copy_(emb_mat)\r\n",
    "        self.bi_GRU = nn.GRU(120, 256, 1, batch_first=True, bidirectional=True)\r\n",
    "        self.attention = AttentionLayer(512, device)\r\n",
    "        self.base_embedding = BaseEmbedding_1(vocab_size, pad_idx)\r\n",
    "\r\n",
    "        # define layer for the structure embedding\r\n",
    "\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.Linear(512, 128),\r\n",
    "            nn.Dropout(),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(128, 2),\r\n",
    "            nn.Softmax(dim=-1)\r\n",
    "        )\r\n",
    "    \r\n",
    "    def forward(self, x, deps):\r\n",
    "        # x -> [batch_size, max_seq_len]        | deps -> [batch_size, max_seq_len, K]\r\n",
    "        x = self.emb(x.long())\r\n",
    "\r\n",
    "        # add the deps embedding here and append to the X embeddings :D\r\n",
    "        deps = self.base_embedding(deps)\r\n",
    "\r\n",
    "        # concat them :D\r\n",
    "        x = torch.cat((x, deps), dim=-1)\r\n",
    "\r\n",
    "        outputs, hidden = self.bi_GRU(x)\r\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\r\n",
    "        \r\n",
    "        # pass to the attention module\r\n",
    "        scores, context = self.attention(outputs)\r\n",
    "        \r\n",
    "        return scores, self.classifier(context)"
   ],
   "outputs": [],
   "metadata": {
    "id": "3xYrdxJpS5KS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AttentionLayer(nn.Module):\r\n",
    "    def __init__(self, emb_dim, device):\r\n",
    "        super(AttentionLayer, self).__init__()\r\n",
    "    \r\n",
    "        # define attentiton layer here\r\n",
    "        self.att_layer = torch.randn((emb_dim, 1), requires_grad=True).to(device)\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        # x.shape = [batch, seq_len, emb_size]\r\n",
    "        x_p = torch.tanh(x)\r\n",
    "        x_p = torch.bmm(x_p, self.att_layer.repeat((x_p.shape[0], 1, 1)))\r\n",
    "        x_p = torch.functional.F.softmax(x_p, dim=-2)\r\n",
    "        return x_p, torch.tanh(torch.bmm(x.transpose(1, 2), x_p).squeeze(-1))\r\n",
    "\r\n",
    "class BaseEmbedding_1(nn.Module):\r\n",
    "    def __init__(self, vocab_size, pad_idx):\r\n",
    "        super(BaseEmbedding_1, self).__init__()\r\n",
    "\r\n",
    "        # load glove and initialize\r\n",
    "        if not os.path.exists(base_dir+'cache/biGRU_embeddings.pt'):\r\n",
    "            glove_dict = {}\r\n",
    "\r\n",
    "            with open(base_dir+'datasets/glove.6B.100d.txt', 'r', encoding='utf-8') as fin:\r\n",
    "                for line in fin.readlines():\r\n",
    "                    line = line.split(' ')\r\n",
    "                    glove_dict[line[0]] = torch.tensor(list(map(float, line[1:])))\r\n",
    "\r\n",
    "            # create embedding matrix\r\n",
    "            emb_mat = torch.randn((vocab_size, 100))\r\n",
    "            for word, w_emb in glove_dict.items():\r\n",
    "                emb_mat[vocab.encode(word)[1]] = w_emb\r\n",
    "            torch.save(emb_mat, base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        else:\r\n",
    "            emb_mat = torch.load(base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        \r\n",
    "        # define layers\r\n",
    "        self.emb = nn.Embedding(vocab_size, 100, padding_idx=pad_idx)\r\n",
    "        self.emb.weight.data.copy_(emb_mat)\r\n",
    "        self.bi_GRU = nn.GRU(100, 50, 1, batch_first=True, bidirectional=True)\r\n",
    "\r\n",
    "    def forward(self, deps):\r\n",
    "        # deps -> [batch_size, max_seq_len, K] -> the focus us on K\r\n",
    "        deps_emb = torch.empty((deps.shape[0], deps.shape[1], 100)).to(device)\r\n",
    "\r\n",
    "        for i in range(deps.size(1)):\r\n",
    "            x = self.emb(deps[:, i, :].long())\r\n",
    "            outputs, hidden = self.bi_GRU(x)\r\n",
    "            # hidden -> [batch_size, emb_size (20)]\r\n",
    "            hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\r\n",
    "            deps_emb[:, i, :] = hidden\r\n",
    "\r\n",
    "        return deps_emb\r\n",
    "\r\n",
    "\r\n",
    "class BaseClassifier_2(nn.Module):\r\n",
    "    def __init__(self, vocab_size, pad_idx, vocab):\r\n",
    "        super(BaseClassifier_2, self).__init__()\r\n",
    "        \r\n",
    "        # load glove and initialize\r\n",
    "        if not os.path.exists(base_dir+'cache/biGRU_embeddings.pt'):\r\n",
    "            glove_dict = {}\r\n",
    "\r\n",
    "            with open(base_dir+'datasets/glove.6B.100d.txt', 'r', encoding='utf-8') as fin:\r\n",
    "                for line in fin.readlines():\r\n",
    "                    line = line.split(' ')\r\n",
    "                    glove_dict[line[0]] = torch.tensor(list(map(float, line[1:])))\r\n",
    "\r\n",
    "            # create embedding matrix\r\n",
    "            emb_mat = torch.randn((vocab_size, 100))\r\n",
    "            for word, w_emb in glove_dict.items():\r\n",
    "                emb_mat[vocab.encode(word)[1]] = w_emb\r\n",
    "            torch.save(emb_mat, base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        else:\r\n",
    "            emb_mat = torch.load(base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        \r\n",
    "        # define layers\r\n",
    "        self.emb = nn.Embedding(vocab_size, 100, padding_idx=pad_idx)\r\n",
    "        self.emb.weight.data.copy_(emb_mat)\r\n",
    "\r\n",
    "        self.bi_GRU = nn.GRU(100, 256, 1, batch_first=True, bidirectional=True)\r\n",
    "        self.attention = AttentionLayer(512, device)\r\n",
    "        self.base_embedding = BaseEmbedding_1(vocab_size, pad_idx)\r\n",
    "        self.emb_nn = nn.Linear(200, 100)\r\n",
    "\r\n",
    "        # define layer for the structure embedding\r\n",
    "\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.Linear(512, 128),\r\n",
    "            nn.Dropout(),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(128, 2),\r\n",
    "            nn.Softmax(dim=-1)\r\n",
    "        )\r\n",
    "    \r\n",
    "    def forward(self, x, deps):\r\n",
    "        # x -> [batch_size, max_seq_len]        | deps -> [batch_size, max_seq_len, K]\r\n",
    "        x = self.emb(x.long())\r\n",
    "\r\n",
    "        # add the deps embedding here and append to the X embeddings :D\r\n",
    "        deps = self.base_embedding(deps)\r\n",
    "\r\n",
    "        # concat them :D\r\n",
    "        x = torch.cat((x, deps), dim=-1)\r\n",
    "        x = self.emb_nn(x)\r\n",
    "\r\n",
    "        outputs, hidden = self.bi_GRU(x)\r\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\r\n",
    "        \r\n",
    "        # pass to the attention module\r\n",
    "        scores, context = self.attention(outputs)\r\n",
    "        \r\n",
    "        return scores, self.classifier(context)"
   ],
   "outputs": [],
   "metadata": {
    "id": "GSmVrlnagiAP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def test_on_data(data_loader, model):\r\n",
    "    model.eval()\r\n",
    "    true_labels = []\r\n",
    "    pred_labels = []\r\n",
    "    \r\n",
    "    with torch.no_grad():\r\n",
    "        for i, data in enumerate(data_loader):\r\n",
    "            # get the inputs; data is a list of [inputs, labels]\r\n",
    "            inputs, labels, deps = data\r\n",
    "\r\n",
    "            # forward + backward + optimize\r\n",
    "            _, outputs = net(inputs.to(device), deps.to(device))\r\n",
    "            \r\n",
    "            true_labels.extend(labels.tolist())\r\n",
    "            pred_labels.extend(torch.argmax(outputs, dim=-1).tolist())\r\n",
    "\r\n",
    "    # get confusion matrix\r\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, pred_labels).ravel()\r\n",
    "    return (tp + tn) / (tn + fp + fn + tp)"
   ],
   "outputs": [],
   "metadata": {
    "id": "VdHYPlUyQFYU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name = 'SEDT_newIMDB_True.pt'\r\n",
    "\r\n",
    "if not os.path.exists(base_dir+'cache/' + model_name):\r\n",
    "    vocab_size = len(dataset.tokenizer.get_vocab())\r\n",
    "    net = BaseClassifier_1(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).to(device)\r\n",
    "    criterion = nn.CrossEntropyLoss()\r\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\r\n",
    "    print('Start training ...')\r\n",
    "    for epoch in range(10):\r\n",
    "        net.train()\r\n",
    "\r\n",
    "        for i, data in enumerate(train_loader):\r\n",
    "            # get the inputs; data is a list of [inputs, labels]\r\n",
    "            inputs, labels, deps = data\r\n",
    "\r\n",
    "            # zero the parameter gradients\r\n",
    "            optimizer.zero_grad()\r\n",
    "\r\n",
    "            # forward + backward + optimize\r\n",
    "            _, outputs = net(inputs.to(device), deps.to(device))\r\n",
    "            # outputs = net(inputs.to(device))\r\n",
    "            loss = criterion(outputs, labels.to(device))\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "        # test on data\r\n",
    "        with torch.no_grad():\r\n",
    "            print('[%d/%d] Validation = %0.4f | Test = %0.4f' %(\r\n",
    "                epoch + 1,\r\n",
    "                30,\r\n",
    "                test_on_data(valid_loader, net),\r\n",
    "                test_on_data(test_loader, net)\r\n",
    "            ))\r\n",
    "\r\n",
    "    print('Finished Training')\r\n",
    "    torch.save(net.state_dict(), base_dir+'cache/' + model_name)\r\n",
    "else:\r\n",
    "    vocab_size = len(dataset.tokenizer.get_vocab())\r\n",
    "    net = BaseClassifier_1(vocab_size, dataset.tokenizer.encode('<pad>')[1], dataset.tokenizer).to(device)\r\n",
    "    net.load_state_dict(torch.load(base_dir+'cache/' + model_name))\r\n",
    "    print('Model Loaded')\r\n",
    "    print(test_on_data(test_loader, net))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "z8X5ihRQj-Uz",
    "outputId": "150680db-3aef-43dd-c18f-26c1ad400fdf",
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class AttentionLayer(nn.Module):\r\n",
    "    def __init__(self, emb_dim, device):\r\n",
    "        super(AttentionLayer, self).__init__()\r\n",
    "    \r\n",
    "        # define attentiton layer here\r\n",
    "        self.att_layer = torch.randn((emb_dim, 1), requires_grad=True).to(device)\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        # x.shape = [batch, seq_len, emb_size]\r\n",
    "        x_p = torch.tanh(x)\r\n",
    "        x_p = torch.bmm(x_p, self.att_layer.repeat((x_p.shape[0], 1, 1)))\r\n",
    "        x_p = torch.functional.F.softmax(x_p, dim=-2)\r\n",
    "        return x_p, torch.tanh(torch.bmm(x.transpose(1, 2), x_p).squeeze(-1))\r\n",
    "\r\n",
    "class BaseEmbedding_1(nn.Module):\r\n",
    "    def __init__(self, vocab_size, pad_idx):\r\n",
    "        super(BaseEmbedding_1, self).__init__()\r\n",
    "\r\n",
    "        # load glove and initialize\r\n",
    "        if not os.path.exists(base_dir+'cache/biGRU_embeddings.pt'):\r\n",
    "            glove_dict = {}\r\n",
    "\r\n",
    "            with open(base_dir+'datasets/glove.6B.100d.txt', 'r', encoding='utf-8') as fin:\r\n",
    "                for line in fin.readlines():\r\n",
    "                    line = line.split(' ')\r\n",
    "                    glove_dict[line[0]] = torch.tensor(list(map(float, line[1:])))\r\n",
    "\r\n",
    "            # create embedding matrix\r\n",
    "            emb_mat = torch.randn((vocab_size, 100))\r\n",
    "            for word, w_emb in glove_dict.items():\r\n",
    "                emb_mat[vocab.encode(word)[1]] = w_emb\r\n",
    "            torch.save(emb_mat, base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        else:\r\n",
    "            emb_mat = torch.load(base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        \r\n",
    "        # define layers\r\n",
    "        self.emb = nn.Embedding(vocab_size, 100, padding_idx=pad_idx)\r\n",
    "        self.emb.weight.data.copy_(emb_mat)\r\n",
    "        self.bi_GRU = nn.GRU(100, 50, 1, batch_first=True, bidirectional=True)\r\n",
    "\r\n",
    "    def forward(self, deps):\r\n",
    "        # deps -> [batch_size, max_seq_len, K] -> the focus us on K\r\n",
    "        deps_emb = torch.empty((deps.shape[0], deps.shape[1], 100)).to(device)\r\n",
    "\r\n",
    "        for i in range(deps.size(1)):\r\n",
    "            x = self.emb(deps[:, i, :].long())\r\n",
    "            outputs, hidden = self.bi_GRU(x)\r\n",
    "            # hidden -> [batch_size, emb_size (20)]\r\n",
    "            hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\r\n",
    "            deps_emb[:, i, :] = hidden\r\n",
    "\r\n",
    "        return deps_emb\r\n",
    "\r\n",
    "\r\n",
    "class BERT_Classifier(nn.Module):\r\n",
    "    def __init__(self, vocab_size, pad_idx, vocab, bert_model):\r\n",
    "        super(BaseClassifier_2, self).__init__()\r\n",
    "        \r\n",
    "        # load glove and initialize\r\n",
    "        if not os.path.exists(base_dir+'cache/biGRU_embeddings.pt'):\r\n",
    "            glove_dict = {}\r\n",
    "\r\n",
    "            with open(base_dir+'datasets/glove.6B.100d.txt', 'r', encoding='utf-8') as fin:\r\n",
    "                for line in fin.readlines():\r\n",
    "                    line = line.split(' ')\r\n",
    "                    glove_dict[line[0]] = torch.tensor(list(map(float, line[1:])))\r\n",
    "\r\n",
    "            # create embedding matrix\r\n",
    "            emb_mat = torch.randn((vocab_size, 100))\r\n",
    "            for word, w_emb in glove_dict.items():\r\n",
    "                emb_mat[vocab.encode(word)[1]] = w_emb\r\n",
    "            torch.save(emb_mat, base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        else:\r\n",
    "            emb_mat = torch.load(base_dir+'cache/biGRU_embeddings.pt')\r\n",
    "        \r\n",
    "        # define layers\r\n",
    "        self.emb = nn.Embedding(vocab_size, 100, padding_idx=pad_idx)\r\n",
    "        self.emb.weight.data.copy_(emb_mat)\r\n",
    "\r\n",
    "        self.bert = bert_model\r\n",
    "\r\n",
    "        # define layer for the structure embedding\r\n",
    "\r\n",
    "        self.classifier = nn.Sequential(\r\n",
    "            nn.Linear(512, 128),\r\n",
    "            nn.Dropout(),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(128, 2),\r\n",
    "            nn.Softmax(dim=-1)\r\n",
    "        )\r\n",
    "    \r\n",
    "    def forward(self, x, deps, input_mask):\r\n",
    "        # x -> [batch_size, max_seq_len]        | deps -> [batch_size, max_seq_len, K]\r\n",
    "        x = self.bert(x, \r\n",
    "                    attention_mask=input_mask, \r\n",
    "                    output_hidden_states=True)\r\n",
    "        x = x.last_hidden_state\r\n",
    "\r\n",
    "        # add the deps embedding here and append to the X embeddings :D\r\n",
    "        deps = self.base_embedding(deps)\r\n",
    "\r\n",
    "        # concat them :D\r\n",
    "        x = torch.cat((x, deps), dim=-1)\r\n",
    "        x = self.emb_nn(x)\r\n",
    "\r\n",
    "        outputs, hidden = self.bi_GRU(x)\r\n",
    "        hidden = torch.cat((hidden[0], hidden[1]), dim=-1)\r\n",
    "        \r\n",
    "        # pass to the attention module\r\n",
    "        scores, context = self.attention(outputs)\r\n",
    "        \r\n",
    "        return scores, self.classifier(context)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import DistilBertForSequenceClassification, AdamW, BertConfig\r\n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\r\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \r\n",
    "# linear classification layer on top. \r\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\r\n",
    "    PRE_TRAINED_MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\r\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\r\n",
    "                    # You can increase this for multi-class tasks.   \r\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\r\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\r\n",
    ")\r\n",
    "\r\n",
    "# Tell pytorch to run this model on the GPU.\r\n",
    "model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "optimizer = AdamW(model.parameters(),\r\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\r\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
    "                )\r\n",
    "from transformers import get_linear_schedule_with_warmup\r\n",
    "\r\n",
    "# Number of training epochs (authors recommend between 2 and 4)\r\n",
    "epochs = 2\r\n",
    "\r\n",
    "# Total number of training steps is number of batches * number of epochs.\r\n",
    "total_steps = len(train_loader) * epochs\r\n",
    "\r\n",
    "# Create the learning rate scheduler.\r\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\r\n",
    "                                            num_training_steps = total_steps)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "# Function to calculate the accuracy of our predictions vs labels\r\n",
    "def flat_accuracy(preds, labels):\r\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
    "    labels_flat = labels.flatten()\r\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss_values = []\r\n",
    "\r\n",
    "for epoch_i in range(0, epochs):\r\n",
    "\r\n",
    "    print(\"\")\r\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
    "    print('Training...')\r\n",
    "    total_loss = 0\r\n",
    "    model.train()\r\n",
    "    for step, batch in enumerate(train_loader):\r\n",
    "\r\n",
    "        b_input_ids = batch[0].to(device)\r\n",
    "        b_input_mask = batch[1].to(device)\r\n",
    "        b_labels = batch[2].unsqueeze(1).to(device)\r\n",
    "\r\n",
    "        model.zero_grad()        \r\n",
    "\r\n",
    "        outputs = model(b_input_ids, \r\n",
    "                    attention_mask=b_input_mask, \r\n",
    "                    labels=b_labels)\r\n",
    "\r\n",
    "        loss = outputs[0]\r\n",
    "        total_loss += loss.item()\r\n",
    "        loss.backward()\r\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
    "        optimizer.step()\r\n",
    "        scheduler.step()\r\n",
    "    avg_train_loss = total_loss / len(train_loader)            \r\n",
    "    loss_values.append(avg_train_loss)\r\n",
    "\r\n",
    "    print(\"\")\r\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))        \r\n",
    "\r\n",
    "    print(\"\")\r\n",
    "    print(\"Running Validation...\")\r\n",
    "\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    # Tracking variables \r\n",
    "    eval_loss, eval_accuracy = 0, 0\r\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\r\n",
    "    for batch in test_loader:\r\n",
    "        batch = tuple(t.to(device) for t in batch)\r\n",
    "        b_input_ids, b_input_mask, b_labels = batch\r\n",
    "        with torch.no_grad():        \r\n",
    "            outputs = model(b_input_ids, \r\n",
    "                            attention_mask=b_input_mask)\r\n",
    "        logits = outputs[0]\r\n",
    "        logits = logits.detach().cpu().numpy()\r\n",
    "        label_ids = b_labels.to('cpu').numpy()\r\n",
    "        \r\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n",
    "        eval_accuracy += tmp_eval_accuracy\r\n",
    "        nb_eval_steps += 1\r\n",
    "\r\n",
    "    print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\r\n",
    "\r\n",
    "print(\"\")\r\n",
    "print(\"Training complete!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eval_loss, eval_accuracy = 0, 0\r\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\r\n",
    "for batch in test_loader:\r\n",
    "    batch = tuple(t.to(device) for t in batch)\r\n",
    "    b_input_ids, b_input_mask, b_labels = batch\r\n",
    "    with torch.no_grad():        \r\n",
    "        outputs = model(b_input_ids, \r\n",
    "                        attention_mask=b_input_mask)\r\n",
    "    logits = outputs[0]\r\n",
    "    logits = logits.detach().cpu().numpy()\r\n",
    "    label_ids = b_labels.to('cpu').numpy()\r\n",
    "    \r\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n",
    "    eval_accuracy += tmp_eval_accuracy\r\n",
    "    nb_eval_steps += 1\r\n",
    "\r\n",
    "print(\"  Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\r\n",
    "    model = model.eval()\r\n",
    "\r\n",
    "    losses = []\r\n",
    "    correct_predictions = 0\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for d in data_loader:\r\n",
    "            input_ids = d[0].to(device).long()\r\n",
    "            attention_mask = d[2].to(device).long()\r\n",
    "            targets = d[1].to(device).long()\r\n",
    "\r\n",
    "            outputs = model(\r\n",
    "            input_ids=input_ids,\r\n",
    "            attention_mask=attention_mask\r\n",
    "            )\r\n",
    "            _, preds = torch.max(outputs, dim=1)\r\n",
    "\r\n",
    "            loss = loss_fn(outputs, targets)\r\n",
    "\r\n",
    "            correct_predictions += torch.sum(preds == targets)\r\n",
    "            losses.append(loss.item())\r\n",
    "\r\n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\r\n",
    "\r\n",
    "EPOCHS = 10\r\n",
    "\r\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\r\n",
    "total_steps = len(train_loader) * EPOCHS\r\n",
    "\r\n",
    "scheduler = get_linear_schedule_with_warmup(\r\n",
    "  optimizer,\r\n",
    "  num_warmup_steps=0,\r\n",
    "  num_training_steps=total_steps\r\n",
    ")\r\n",
    "\r\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "punctuation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python385jvsc74a57bd03283be21a077194c2bf83e4143078281de154c938414858b4f1a9e29f5860274",
   "display_name": "Python 3.8.5 64-bit ('pytorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "097695a91986466c848d6c31bf925898": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15fd4d631ae34c36be18fa08f9ba2457": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_59ee1a52c1d7433189e015b8a21d7d1b",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4c085f8a4006497c95722f9225748282",
      "value": 28
     }
    },
    "21740080bc6b4e8eba7b412261c37e6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6368c08d69b44d75a9ab5eb96f43d86e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5a8df54d5d9d4207acd7c9a1780a9d9c",
      "value": " 28.0/28.0 [00:00&lt;00:00, 144B/s]"
     }
    },
    "28195f49042d4151b8191ceee753749c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f17cf9c83674d1fb822329fa6d5e015": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb48191c59714b58ba38e49e16ea77e3",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c9240036b41442c082fec7162d249713",
      "value": 231508
     }
    },
    "40449512fcae427eabfb6af1f5e4908c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c085f8a4006497c95722f9225748282": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4fecbe7bf0614d9094680af0c603d252": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59ee1a52c1d7433189e015b8a21d7d1b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a8df54d5d9d4207acd7c9a1780a9d9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6368c08d69b44d75a9ab5eb96f43d86e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "681e0f4c216d4a69baa4694c985bf18a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96e04dfd07f2429b9b0fbb4cb133a574": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f7d250b016064514a0b45353ccdb34e9",
       "IPY_MODEL_99cfbff75de34278a9201ad8e72b7505"
      ],
      "layout": "IPY_MODEL_b6fa1e8c8243406cbabae8f205e717fb"
     }
    },
    "99cfbff75de34278a9201ad8e72b7505": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_097695a91986466c848d6c31bf925898",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bf51472b91964e83ae23735c2cc692df",
      "value": " 466k/466k [00:00&lt;00:00, 4.99MB/s]"
     }
    },
    "b6fa1e8c8243406cbabae8f205e717fb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb67626da99843a7b8c14fd869e85080": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3f17cf9c83674d1fb822329fa6d5e015",
       "IPY_MODEL_ca68dd3284ac4965baa4a5496ffbc663"
      ],
      "layout": "IPY_MODEL_bda0de59590344708c2b60dca090c687"
     }
    },
    "bda0de59590344708c2b60dca090c687": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf51472b91964e83ae23735c2cc692df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c234c2824a204609be6af46e974881be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_15fd4d631ae34c36be18fa08f9ba2457",
       "IPY_MODEL_21740080bc6b4e8eba7b412261c37e6f"
      ],
      "layout": "IPY_MODEL_681e0f4c216d4a69baa4694c985bf18a"
     }
    },
    "c9240036b41442c082fec7162d249713": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ca68dd3284ac4965baa4a5496ffbc663": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40449512fcae427eabfb6af1f5e4908c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_28195f49042d4151b8191ceee753749c",
      "value": " 232k/232k [00:00&lt;00:00, 963kB/s]"
     }
    },
    "cb48191c59714b58ba38e49e16ea77e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7d250b016064514a0b45353ccdb34e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fecbe7bf0614d9094680af0c603d252",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fe7c4a0c4694467fa7024d1b07459084",
      "value": 466062
     }
    },
    "fe7c4a0c4694467fa7024d1b07459084": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}